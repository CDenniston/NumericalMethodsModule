{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d183dc-0a1d-4959-8638-35aade4a440f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Faster Convergence and Newton's Method\n",
    "\n",
    "In the previous section, we found that the magnitude of $g'(p)$ plays a significant role in how fast a fixed point iteration scheme converges.  In this section we address the quesion of convergence when $g'(p)=0$. \n",
    "\n",
    "> **Theorem:**\n",
    "> - If $p$ is a solution to $x=g(x)$ with $g'(p)=0$ (assuming $g$ and $g'$ are continuous functions), then there exists $\\delta>0$ such that for all $p_0\\in(p-\\delta,p+\\delta)$, fixed point iteration coverges quadratically (or has order of convergence 2).\n",
    "\n",
    "For the proof, it is not too hard to show that if $g'$ is continuous that if $g'(p)=0$ then there must be some neighborhood of $p$ where $|g'(x)| \\leq k < 1$ (i.e. there exists $\\delta$ where $|g'(x)| \\leq k < 1$ in $(p-\\delta,p+\\delta)$).  You can then use the Mean Value theorem to show that $g(x)$ maps this neighborhood of $p$ onto itself.  As a result, we have the conditions required by our fixed point convergence theorem so that choosing our initial guess in this interval will result in convergence to a unique fixed point.  To see that this give quadratic convergence, note that given $p_i\\in (p-\\delta,p+\\delta)$ we can expand $g(x)$ in a Taylor series about $p$,\n",
    "\n",
    "$$\n",
    "g(p_i)=g(p) + g'(p)(p_i-p) + \\frac{g''(\\xi)}{2}(p_i-p)^2,\n",
    "$$\n",
    "\n",
    "and given that $g(p_i)=p_{i+1}$, $g(p)=p$, and $g'(p)=0$, this gives\n",
    "\n",
    "$$\n",
    "p_{i+1}-p = \\frac{g''(\\xi)}{2}(p_i-p)^2,\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\lim_{i\\rightarrow \\infty} \\frac{|p_{i+1}-p|}{|p_{i}-p|^2} = C =\\frac{g''(\\xi)}{2}.\n",
    "$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "**Example:** A *really* old fixed point scheme for [finding square roots](https://en.wikipedia.org/wiki/Square_root) (a description appears in Book 1 of Metrica by Heron of Alexandria from the first century C.E. but it is also clear that it was actually also widely known several hundred years earlier by ancient Indian and Chinese mathematians and as much as two thousand years earlier by ancient Mesopotanians).  The scheme is based on the idea that if you have a guess $x_i$ for the square root of a number $y$, then $y/x_i$ will be greater/less than the true square root if $x_i$ is too small/large.  As such, the average of $x_i$ and $y/x_i$ should be closer.  The fixed point scheme thus becomes\n",
    "\n",
    "$$\n",
    "p_{i+1}=\\frac{p_i + \\frac{y}{p_i}}{2}.\n",
    "$$\n",
    "\n",
    "If you try this in our `FixPointIteration` function from the previous section, to compute the square $\\sqrt{2}$, you will get the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ae3c0a9-2929-4b2b-acf6-b4f3f445448f",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def FixPointIteration(g, p0, tol, maxN = 100, output = True):\n",
    "    # The p0 argument here is our initial guess\n",
    "    \n",
    "    # print output table headings\n",
    "    if (output):\n",
    "         print(\"         p(i)                 p(i+1)=g( p(i) )\")  \n",
    "\n",
    "    # main loop\n",
    "    for i in range(1,maxN):\n",
    "        p1 = g(p0)\n",
    "\n",
    "        if (output):\n",
    "             print(f\"{p0:>20} {p1:>24}\")\n",
    "\n",
    "        if (abs(p1-p0) < tol) :\n",
    "            print(\"Converged in\", i, \"iterations\")\n",
    "            return p1\n",
    "        else :\n",
    "            p0 = p1\n",
    "    \n",
    "    # if we finish the main loop without returning from the FixPointIteration function, we have failed.  :( \n",
    "    print(f\"Error: Could not find fixed point to within {tol} in {maxN} iterations. Returning best guess so far.\")\n",
    "    return p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdd81c56-2dfc-4574-91f8-10dfcb2c8900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         p(i)                 p(i+1)=g( p(i) )\n",
      "                 1.0                      1.5\n",
      "                 1.5       1.4166666666666665\n",
      "  1.4166666666666665       1.4142156862745097\n",
      "Converged in 3 iterations\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def my_g(x):\n",
    "    return (x+2/x)/2\n",
    "    \n",
    "root = FixPointIteration(my_g,1.0, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcedce-ec10-4820-a788-b6f5774c31fc",
   "metadata": {},
   "source": [
    "Note that the actual error is in the seventh digit here.  The same result in cuniform can be found on a nearly four thousand year old clay tablet inscribed by an ancient Mespotanian.  They also used base 60 arithmetic so that they were able to obtain the same $3\\times 10^{-6}$ accuracy with three base-sixty digits.  Any time you feel like complaining about modern computers, keep in mind there was some dude sitting on the banks of the Euphrates river doing these calculations in the mud, with a reed, nearly four thousand year before anyone even thought about electronics.\n",
    "\n",
    "This example converges *much* faster than the examples from the previous sections.  This is due to the fact that\n",
    "\n",
    "$$\n",
    "g'(p) = \\frac{1}{2}\\left(1-\\frac{y}{p^2} \\right) = 0,\n",
    "$$\n",
    "\n",
    "where the last equality comes from the fact that the fixed point $p$ is, by construction, $\\sqrt{y}$.  Convergence to near numerical precision in just a few interations, rather than 10-20+ iterations for just a few digits of accuracy for a general fixed point scheme, is one of the reasons that quadratic convergence can be vitaly important.\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd5e64-834e-4272-8171-d6136804dd9f",
   "metadata": {},
   "source": [
    "Suppose we want to solve $f(x)=0$ using a fixed point iteration scheme $p_{i+1}=g(p_i)$ with $g(x) = x - \\phi(x)f(x)$.  If we want quadratic convergence, what constraints does this put on our choice for $\\phi(x)$?\n",
    "\n",
    "We need to have $g'(p)=0$ and $g'(x)=1-\\phi'(x)f(x)-\\phi(x)f'(x)$ so this means that we must have $1-\\phi'(p)f(p)-\\phi(p)f'(p)=0$.  However, $f(p)=0$ so this implies that, assuming $f'(p)\\neq 0$,\n",
    "\n",
    "$$\n",
    "\\phi(p) = \\frac{1}{f'(p)}.\n",
    "$$\n",
    "\n",
    "This only constrains the value of $\\phi$ at one point $x=p$.  However, if we just take $\\phi(x)=\\frac{1}{f'(x)}$ for all $x$ (not just at $p$) then this gives us \n",
    "\n",
    "> **Newton's Method**: Given $p_0$, then\n",
    ">\n",
    "> $$p_{i+1}=p_i -\\frac{f(p_i)}{f'(p_i)},\\qquad i=1,2,3,\\cdots$$\n",
    "\n",
    "However, note that this is *not* the only possible choice.\n",
    "\n",
    "**Example:** Let's use Newton's Method to find the roots of the function $f(x)=e^{-x}\\cos{x}$.  We did this already using bisection, so let's compare the relative speed of convergence for the two methods.  We already know the roots are near $\\pm 1.57$, so let's try starting at $\\pm 1.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6ce1ac9-38d6-451f-9146-7c53bd921581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         p(i)                 p(i+1)=g( p(i) )\n",
      "                 1.5        1.566218938583142\n",
      "   1.566218938583142       1.5707755014615041\n",
      "  1.5707755014615041       1.5707963263612141\n",
      "  1.5707963263612141       1.5707963267948966\n",
      "Converged in 4 iterations\n",
      "Best guess for first root is 1.5707963267948966\n",
      "\n",
      "         p(i)                 p(i+1)=g( p(i) )\n",
      "                -1.5      -1.5763276044911358\n",
      " -1.5763276044911358      -1.5708266977374767\n",
      " -1.5708266977374767      -1.5707963277172534\n",
      " -1.5707963277172534      -1.5707963267948966\n",
      "Converged in 4 iterations\n",
      "Best guess for first root is  -1.5707963267948966\n"
     ]
    }
   ],
   "source": [
    "def my_f(x):\n",
    "    return np.exp(-x)*np.cos(x)\n",
    "\n",
    "def my_fp(x):\n",
    "    return -np.exp(-x)*np.cos(x)-np.exp(-x)*np.sin(x)\n",
    "\n",
    "def Newt_g(x):\n",
    "    return x - my_f(x)/my_fp(x)\n",
    "\n",
    "root = FixPointIteration(Newt_g, 1.5, 1e-9)\n",
    "print(f\"Best guess for first root is {root}\\n\")\n",
    "root = FixPointIteration(Newt_g, -1.5, 1e-9)\n",
    "print(\"Best guess for first root is \", root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1d2ad-e43c-48c0-8891-f588c74d867e",
   "metadata": {},
   "source": [
    "We see that Newton's method finds our roots *very* quickly.  Recall that Bisection required 30 iterations to obtain a similar level of accuracy and here our actual accuracy is better than the tolerance and is actually the full 15 digits which would have required bisection a total of 50 iterations to achieve.  The rapid convergence, within a few iterations, is typical for Newton's method.  You might argue that we started quite close to the root so it is not a fair comparison.  If we start further away, we get  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8255dcf2-dfb8-4811-a8ea-6798a389a463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         p(i)                 p(i+1)=g( p(i) )\n",
      "                 2.0       1.1561465305920122\n",
      "  1.1561465305920122        1.461784349729494\n",
      "   1.461784349729494       1.5604334703205522\n",
      "  1.5604334703205522       1.5706904028378534\n",
      "  1.5706904028378534       1.5707963155765963\n",
      "  1.5707963155765963       1.5707963267948966\n",
      "  1.5707963267948966       1.5707963267948966\n",
      "Converged in 7 iterations\n",
      "Best guess for first root is 1.5707963267948966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root = FixPointIteration(Newt_g, 2.0, 1e-9)\n",
    "print(f\"Best guess for first root is {root}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9cb40-d826-44a4-a310-33b1afa2fece",
   "metadata": {},
   "source": [
    "This did require a couple more iterations, but it is still fairly quick.  However, it is important to note that the true quadratic convergence is an asympotic property of the series that is only strictly true once we are very close to the root.  In fact, if we are too far from the root we may not converge at all.  Unfortunatly how \"close\" is close enough is not generally something we know ahead of time (i.e. determining $\\delta$ in the theorem at the beginning of this section probably requires as much, if not more, work as determining the value for the root).  \n",
    "\n",
    "In summary we can construct the following table comparing bisection and Newton's method:\n",
    "\n",
    "| Method      | Pros | Cons     |\n",
    "| :---        |    :----   |   :--- |\n",
    "| Bisection   | - good globally | - slow, linear convergence |\n",
    "|             | - always works  | - requres root bracketing  |\n",
    "| Newton      | - fast quadratic convergence        | - initial guess must be \"close\"  |\n",
    "|             |                                     | - needs derivative $f'(x)$  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f0fdc-5dca-4be8-b6bb-2bea5b91b601",
   "metadata": {},
   "source": [
    "To use Newton's method, we need to know the derivative $f'(x)$.  However, sometimes we don't know (or don't want to evaluate) $f'(x)$.  In that case, we could try approximating $f'(x)$ by \n",
    "\n",
    "$$\n",
    "f'(p_i)\\approx \\frac{f(p_i)-f(p_{i-1})}{p_i-p_{i-1}}.\n",
    "$$\n",
    "\n",
    "If we are close enough the to root to get convergence, as $i\\rightarrow \\infty$ then $p_{i-1}\\rightarrow p_i$ so that this expression should approach the true derivative at $f'(p_i)$.  This gives us\n",
    "\n",
    "> **Secant Method**: Given $p_0$ and $p_1$ then\n",
    ">\n",
    "> $$ p_{i+1}=p_i -\\frac{f(p_i)}{\\left[\\frac{f(p_i)-f(p_{i-1})}{p_i-p_{i-1}}\\right]},\\qquad i=1,2,3,\\cdots $$\n",
    "\n",
    "There is a cost in doing this in that i) we need two initial guesses and ii) the rate of convergence turns out to be $\\alpha \\approx 1.618$ which is less than the quadratic convergence of Newton's method but still much better than linear convergence.\n",
    "\n",
    "It is also reasonable to ask: Can we do better?  This is the subject of the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
