{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f9b1b0-87fc-4427-bd75-183f44f09643",
   "metadata": {},
   "source": [
    "### LU Factorization\n",
    "\n",
    "Suppose that we had an $n\\times n$ matrix $A=LU$ where $L$ is lower triangular and $U$ is upper triagular.  We could use this factorization to solve $Ax=b$ as follows:\n",
    "\n",
    "1. Rewrite $Ax=b$ as\n",
    "   \n",
    "   $$\n",
    "   LU x = b\n",
    "   $$ (lueqns)\n",
    "   \n",
    "2. Define $y$ by the equation\n",
    "\n",
    "   $$\n",
    "    U x=y\n",
    "   $$\n",
    "\n",
    "    Note that $LU x=Ly$ so Eq. {eq}`lueqns` becomes\n",
    "\n",
    "   $$\n",
    "   Ly=b\n",
    "    $$\n",
    "3. Solve $Ly=b$ for $y$ using **forward** substitution.\n",
    "4. Solve $Ux=y$ for $x$ using *backward* substitution using the $y$ from the previous step.\n",
    "\n",
    "We saw that backwards substitution was a simple way to solve an upper triangular system of equations that we obtained after Gaussian elimination.  *Forward* substitution is the analagous way of solving a lower triangular system of equations.\n",
    "\n",
    "How much does this cost?  We saw in the previous section that backward substitution cost $\\sim \\frac{n^2}{2}$ flops.  As forward substitution is just the same sequance of operations just from top to bottom rather than bottom to top, it should also cost $\\sim \\frac{n^2}{2}$ flops.  Which gives us a total of $n^2$ flops which is *much* faster than the $\\frac{n^3}{3}$ required for the full Gaussian elimination and back substitution.\n",
    "\n",
    "The following example illustrates that steps\n",
    "\n",
    "**Example**  Given the following factorization of a matrix, \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\underbrace{\\left[ {\\begin{array}{ccc}\n",
    "    2 &  6 & 2 \\\\\n",
    "    -3 &  -8 & 0 \\\\\n",
    "     4 &  9 & 2 \\\\\n",
    "  \\end{array} } \\right]}_{A} =\n",
    "  \\underbrace{\\left[ {{\\begin{array}{ccc}\n",
    "  2 & 0 & 0 \\\\\n",
    "  -3 & 1 & 0 \\\\\n",
    "  4 & -3 & 7 \\\\\n",
    "  \\end{array}}} \\right]}_{L} \n",
    "  \\underbrace{\\left[ {{\\begin{array}{ccc}\n",
    "  1 & 3 & 1 \\\\\n",
    "  0 & 1 & 3 \\\\\n",
    "  0 & 0 & 1 \\\\\n",
    "  \\end{array}}} \\right]}_{U} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which you can easily verify by multiplying out the $L$ and $U$, we want to solve $Ax=b$ with\n",
    "\n",
    "$$\n",
    "b=\\left[\\begin{array}{c}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\end{array}\\right].\n",
    "$$\n",
    "\n",
    "We first need to solve $Ly=b$ for $y$, i.e.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[ {{\\begin{array}{ccc}\n",
    "  2 & 0 & 0 \\\\\n",
    "  -3 & 1 & 0 \\\\\n",
    "  4 & -3 & 7 \\\\\n",
    "  \\end{array}}} \\right]\n",
    "  \\left[\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\end{array}\\right]=\n",
    "\\left[\\begin{array}{c}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\end{array}\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "using forward substitution:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "2 y_1 &= 2\\quad \\Rightarrow y_1=1\\\\\n",
    "y_2 &= \\frac{2+3y_1}{1}=5\\\\\n",
    "y_3 &= \\frac{3-(4y_1-3y_2)}{7}=2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "then we solve $U x=y$ for $x$, i.e.\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\\left[{{\\begin{array}{ccc}\n",
    "  1 & 3 & 1 \\\\\n",
    "  0 & 1 & 3 \\\\\n",
    "  0 & 0 & 1 \\\\\n",
    "  \\end{array}}} \\right]\n",
    "  \\left[\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\end{array}\\right]=\n",
    "\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "5 \\\\\n",
    "2 \\\\\n",
    "\\end{array}\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "using backward substitution:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_3&=2\\\\\n",
    "x_2&=5-3 x_3 = -1\\\\\n",
    "x_1&=1-x_3-3x_2 = 2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So the solution is \n",
    "\n",
    "$$\n",
    "x=\\left[\\begin{array}{c}\n",
    "2 \\\\\n",
    "-1 \\\\\n",
    "2 \\\\\n",
    "\\end{array}\\right].\n",
    "$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "This brings up the question of: How do we $LU$ factorize a matrix $A$ and when can we do this?  This is answered in the following theorem.\n",
    "\n",
    "> **Theorem:**\n",
    "> If Gaussian elimination can be performed on $Ax=b$ *without* row interchanges, then $A$ can be factored into $A=LU$, where $U$ is the resulting matrix after forward elimination and $L$ is the lower triangular matrix of multipliers and diagonal elements 1.\n",
    "\n",
    "The theorem should not be a massive surprise as it is literally just a translation of the reverse of the linear operations performed in the inner loop of Gaussian (forward) elimination into matrix algebra.  We illustrate the process in the following example.\n",
    "\n",
    "**Example**  We want to $LU$-factorize the following matrix:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  A=\\left[ {\\begin{array}{cccc}\n",
    "     1 &  1 & 0 &  3 \\\\\n",
    "     2 &  1 & -1 &  1 \\\\\n",
    "     3 &  -1 & -1 &  2 \\\\\n",
    "     -1 &  2 & 3 &  -1 \\\\\n",
    "  \\end{array} } \\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To do so, we first just perform Gaussian elimination to row reduce to get $U$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " {\\begin{array}{cc}\n",
    " E_1 &:\\\\\n",
    " (E_2-\\textcolor{red}{2}E_1)\\rightarrow E_2 &: \\\\\n",
    " (E_3-\\textcolor{red}{3}E_1)\\rightarrow E_3 &: \\\\\n",
    " (E_4-\\textcolor{red}{(-1)}E_1)\\rightarrow E_4 &: \\\\\n",
    " \\end{array}}\\qquad\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "     1 &  1 & 0 &  3 \\\\\n",
    "     0 &  -1 & -1 &  -5 \\\\\n",
    "     0 &  -4 & -1 &  -7 \\\\\n",
    "     0 &  3 & 3 &  2 \\\\\n",
    "  \\end{array} } \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We note the multipliers used (in red above) which we will use to construct $L$ below. Moving on to the next step in row reduction:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " {\\begin{array}{cc}\n",
    " E_1 & :\\\\\n",
    " E_2 & :\\\\\n",
    " (E_3-\\textcolor{blue}{4}E_2)\\rightarrow E_3 &:\\\\\n",
    " (E_4-\\textcolor{blue}{(-3)}E_2)\\rightarrow E_4 &:\\\\\n",
    " \\end{array}}\\qquad \\underbrace{\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "     1 &  1 & 0 &  3 \\\\\n",
    "     0 &  -1 & -1 &  -5 \\\\\n",
    "     0 &  0 & -3 &  13 \\\\\n",
    "     0 &  0 & 0 &  -13 \\\\\n",
    "  \\end{array} } \\right]}_{U}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Normally we would need one final row reduction to reduce the coefficient matrix to an *upper triangular matrix* $U$, but in this case we see that it is already there, and the multiplier needed to reduce the the last row is just $\\textcolor{green}{0}$.  To construct $L$, we just fill in the multipliers used to zero each component into a matrix with diagonal's $1$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " L=\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "     1 &  0 & 0 &  0 \\\\\n",
    "     \\textcolor{red}{2} &  1 & 0 &  0 \\\\\n",
    "     \\textcolor{red}{3} &  \\textcolor{blue}{4} & 1 &  0 \\\\\n",
    "     \\textcolor{red}{-1} &  \\textcolor{blue}{-3} & \\textcolor{green}{0} &  1 \\\\\n",
    "  \\end{array} } \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You can easily verify by direct multiplication of $L$ by $U$ that this does indeed give us back our original matrix $A$.\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "It should be clear from the above that $LU$ factorization and Gaussian elimination are equivalent amounts of work ($\\sim n^3/3$ flops) as they consist of the same operations.  So where do we get any cost savings from using $LU$ factorization to solve linear systems?  The key factor is that now we can solve any number of systems that have the same coefficient matrix, but different right-hand sides (the $b$ in $Ax=b$), while doing the costly $n^3/3$ bit *only once*.  For a $100\\times 100$ system this means that we can solve with roughly $35$ different $b$'s in the same time that it would take to do the full Gaussian elimination and back substitution twice.  For larger systems or a larger number of different $b$'s the savings are even greater.\n",
    "\n",
    "What about row interchanges?  We can describe row interchanges in terms of a *permutation matrix*.\n",
    "\n",
    "> **Definition:** A $n\\times n$ *permutation matrix* $P=[p_{ij}]$ is obtained by rearranging the rows of the identity matrix $I_n$.\n",
    "\n",
    "We can use the permutation matrix as an operator to rearrange the rows of another $n\\times n$ matrix $A$ in the same way that the permutation matrix had its rows rearranged from the identity.\n",
    "\n",
    "**Example**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  P=\\left[ {\\begin{array}{ccc}\n",
    "     1 &  0 & 0  \\\\\n",
    "     0 &  0 & 1 \\\\\n",
    "     0 &  1 & 0 \\\\\n",
    "  \\end{array} } \\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "is obtained from the identity matrix by swapping the second and third rows.  If we multiply a generic $3\\times 3$ matrix $A$ by $P$, it will swap the corresponding rows in $A$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  PA=\\left[ {\\begin{array}{ccc}\n",
    "     1 &  0 & 0  \\\\\n",
    "     0 &  0 & 1 \\\\\n",
    "     0 &  1 & 0 \\\\\n",
    "  \\end{array} } \\right]\n",
    "  \\left[ {\\begin{array}{ccc}\n",
    "     a_{11} &  a_{12} & a_{13}  \\\\\n",
    "     a_{21} &  a_{22} & a_{23} \\\\\n",
    "     a_{31} &  a_{32} & a_{33} \\\\\n",
    "  \\end{array} } \\right]=\n",
    "  \\left[ {\\begin{array}{ccc}\n",
    "     a_{11} &  a_{12} & a_{13}  \\\\\n",
    "     a_{31} &  a_{32} & a_{33} \\\\\n",
    "     a_{21} &  a_{22} & a_{23} \\\\\n",
    "  \\end{array} }\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "Another useful property of permutation matrices is that the inverse $P^{-1}$ exists and $P^{-1}=P^t$, the transpose.\n",
    "\n",
    "If we know all row interchanges that will be used in Gaussian elimination with pivoting, then we could $LU$ factorize $PA$ where $P$ is the permuation matrix corresponding to the row interchanges done in the pivot steps:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "PA&=LU\\\\\n",
    "\\Rightarrow A &= P^{-1}LU = P^tLU\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and for $Ax=b$ we solve $PA x = Pb$ i.e. we need to reorder $b$ too, just as we would have if it had been part of the augmented matrix for Gaussian elimination.  In practice, it is inefficient to store $P$ as it is mostly zeros.  It is better to store the row indices needed to construct $P$, i.e. our previous row index array from partial pivoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263b1657-6f96-44f0-8788-f096f2a44c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cofficient Matrix:\n",
      " [[ 1.  1.  1.  1.]\n",
      " [ 1.  4. 16. 64.]\n",
      " [ 1.  3.  9. 27.]\n",
      " [ 1.  2.  4.  8.]]\n",
      "LU Factors stored in single matrix:\n",
      " [[  1.           1.           1.           1.        ]\n",
      " [  1.           3.          15.          63.        ]\n",
      " [  1.           0.66666667  -2.         -16.        ]\n",
      " [  1.           0.33333333   1.           2.        ]]\n",
      "row index:\n",
      " [0 1 2 3]\n",
      "right hand side [ 3.  0. -5. -2.]\n",
      "y from solving Ly=b [ 3. -3. -6.  2.]\n",
      "solution:\n",
      " [ 4.  3. -5.  1.] \n",
      "\n",
      "Cofficient Matrix:\n",
      " [[ 1.  1.  1.  1.]\n",
      " [ 1.  2.  4.  8.]\n",
      " [ 1.  3.  9. 27.]\n",
      " [ 1.  4. 16. 64.]]\n",
      "LU Factors stored in single matrix:\n",
      " [[  1.           1.           1.           1.        ]\n",
      " [  1.           0.33333333   1.           2.        ]\n",
      " [  1.           0.66666667  -2.         -16.        ]\n",
      " [  1.           3.          15.          63.        ]]\n",
      "row index:\n",
      " [0 3 2 1]\n",
      "right hand side [ 3. -2. -5.  0.]\n",
      "y from solving Ly=b [ 3. -3. -6.  2.]\n",
      "solution:\n",
      " [ 4.  3. -5.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def PLUFactorization(A,n):\n",
    "    # setup our row index array\n",
    "    nrows=np.array(range(0,n),dtype=int)\n",
    "    for k in range(0,n-1):\n",
    "        # select pivot element A[k,k]\n",
    "        pivot = A[nrows[k],k]\n",
    "        prow = nrows[k]\n",
    "        for p in range(k+1,n):\n",
    "            if (abs(A[nrows[p],k]) > abs(pivot)) :\n",
    "                prow = p\n",
    "                pivot = A[nrows[p],k]\n",
    "        if (prow != nrows[k]) :\n",
    "            if (pivot == 0) :\n",
    "                print(\"Singular Matrix Encountered\\n\")\n",
    "                return nrows\n",
    "            # As rows may have been swapped previously, we need to double index nrows\n",
    "            tmp = nrows[nrows[k]]\n",
    "            nrows[nrows[k]]=nrows[nrows[prow]]\n",
    "            nrows[nrows[prow]]=tmp\n",
    "        # Now we loop over i, the rows below the pivot row\n",
    "        for i in range(k+1,n):\n",
    "            m=A[nrows[i],k]/pivot\n",
    "            # The multiplier would normally zero out A[nrows[i],k]. Rather than storing\n",
    "            # the zero we will store the multiplier, as that is what is needed for L:\n",
    "            A[nrows[i],k] = m\n",
    "            # Loop over j, the columns of row i\n",
    "            # The line below is equivalent to the following loop,\n",
    "            # for j in range(k,n+1):\n",
    "            #    A[i,j] -= m*A[k,j]\n",
    "            A[nrows[i], (k+1):] -= m*A[nrows[k], (k+1):]\n",
    "    return nrows\n",
    "\n",
    "def ForwardSubstitution(L,b,n,nrows):\n",
    "    y=np.zeros(n)\n",
    "    # We don't need to do any division here as diagonals of L are one\n",
    "    y[0]=b[nrows[0]]\n",
    "    for k in range(1,n):\n",
    "        y[k]=b[nrows[k]]\n",
    "        for j in range(0,k):\n",
    "            y[k] -= L[nrows[k],j]*y[j]\n",
    "        # Note: L has diagonal elements one that are not stored.  If that\n",
    "        # was not the case we would need to divide by L[n[rows[k],k] here\n",
    "    return y\n",
    "\n",
    "def BackSubstitution(U,y,n,nrows):\n",
    "    x=np.zeros(n)\n",
    "    x[n-1]=y[n-1]/U[nrows[n-1],n-1]\n",
    "    for k in range(n-2,-1, -1):\n",
    "        x[k]=y[k]\n",
    "        for j in range(k+1,n):\n",
    "            x[k] -= U[nrows[k],j]*x[j]\n",
    "        x[k]=x[k]/U[nrows[k],k]\n",
    "    return x\n",
    "\n",
    "# Problem from our original example from Gaussian elimination with 2nd and last row swapped so no row interchanges\n",
    "CoefficientMatrix=np.array([[1,1,1,1],[1,4,16,64],[1,3,9,27],[1,2,4,8]],dtype=np.float64)\n",
    "print(\"Cofficient Matrix:\\n\", CoefficientMatrix)\n",
    "rowsindx=PLUFactorization(CoefficientMatrix,4)\n",
    "print(\"LU Factors stored in single matrix:\\n\", CoefficientMatrix)\n",
    "print(\"row index:\\n\",rowsindx)\n",
    "\n",
    "# Need to swap elements of b too\n",
    "b = np.array([3,0,-5,-2],dtype=np.float64)\n",
    "print(\"right hand side\",b)\n",
    "my_y = ForwardSubstitution(CoefficientMatrix,b,4,rowsindx)\n",
    "print(\"y from solving Ly=b\",my_y)\n",
    "my_x = BackSubstitution(CoefficientMatrix,my_y,4,rowsindx)\n",
    "print(\"solution:\\n\",my_x,\"\\n\")\n",
    "\n",
    "# The problem from our original example that swapped rows during partial pivoting:\n",
    "CoefficientMatrix=np.array([[1,1,1,1],[1,2,4,8],[1,3,9,27],[1,4,16,64]],dtype=np.float64)\n",
    "#CoefficientMatrix=np.array([[1,1,1,1],[1,4,16,64],[1,3,9,27],[1,2,4,8]],dtype=np.float64)\n",
    "print(\"Cofficient Matrix:\\n\", CoefficientMatrix)\n",
    "rowsindx=PLUFactorization(CoefficientMatrix,4)\n",
    "print(\"LU Factors stored in single matrix:\\n\", CoefficientMatrix)\n",
    "print(\"row index:\\n\",rowsindx)\n",
    "\n",
    "b = np.array([3,-2,-5,0],dtype=np.float64)\n",
    "print(\"right hand side\",b)\n",
    "my_y = ForwardSubstitution(CoefficientMatrix,b,4,rowsindx)\n",
    "print(\"y from solving Ly=b\",my_y)\n",
    "my_x = BackSubstitution(CoefficientMatrix,my_y,4,rowsindx)\n",
    "print(\"solution:\\n\",my_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb3835-7969-4903-8656-3c6df06717cd",
   "metadata": {},
   "source": [
    "We see that the $PLU$ factorization routine is almost identical to our previous ForwardElimination routine with partial pivoting. The only difference is that we save the multipliers $m$ in the lower triangular part of the matrix to form $L$ and we pass in just the coefficient matrix (it doesn't have the extra column of the augmented matrix).  The back substitution routine is also very similar except we also need to pass in the right hand side (the $y$ in this case) as it is not stored in the $U$ matrix, unlike the row reduced augmented matrix.\n",
    "\n",
    "Finding the inverse is one case where we need to repeated solve.  The matrix equation, $AA^{-1} = I$ is equivalent to solving the $n$ linear systems\n",
    "\n",
    "$$\n",
    "Ax_1=e_1,\\quad Ax_2=e_2,\\quad \\cdots,\\quad Ax_n=e_n\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "e_j=\\left[{\\begin{array}{c}\n",
    "     0 \\\\\n",
    "     \\vdots \\\\\n",
    "     0 \\\\\n",
    "     1 \\\\\n",
    "     0 \\\\\n",
    "     \\vdots\\\\\n",
    "     0 \\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "where the $1$ is in the $j$'th row and $x_1,\\cdots,x_n$ are the columns of $A^{-1}$.\n",
    "\n",
    "The following example illustrates the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639974ec-4959-46db-9027-3005ad065b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Matrix:\n",
      " [[ 1.  1.  1.  1.]\n",
      " [ 1.  2.  4.  8.]\n",
      " [ 1.  3.  9. 27.]\n",
      " [ 1.  4. 16. 64.]]\n",
      "Inverse of My Matrix:\n",
      " [[ 4.         -6.          4.         -1.        ]\n",
      " [-4.33333333  9.5        -7.          1.83333333]\n",
      " [ 1.5        -4.          3.5        -1.        ]\n",
      " [-0.16666667  0.5        -0.5         0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "# Again from our original example:\n",
    "My_Matrix=np.array([[1,1,1,1],[1,2,4,8],[1,3,9,27],[1,4,16,64]],dtype=np.float64)\n",
    "print(\"My Matrix:\\n\", My_Matrix)\n",
    "rowsindx=PLUFactorization(My_Matrix,4)\n",
    "\n",
    "My_Inverse=np.zeros((4,4))\n",
    "for j in range(0,4):\n",
    "    e_j=np.zeros(4)\n",
    "    e_j[j]=1\n",
    "    my_y = ForwardSubstitution(My_Matrix,e_j,4,rowsindx)\n",
    "    my_x = BackSubstitution(My_Matrix,my_y,4,rowsindx)\n",
    "    My_Inverse[:,j]=my_x\n",
    "\n",
    "print(\"Inverse of My Matrix:\\n\",My_Inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8dbda5-be99-4fe4-a7ba-f58534d17e3f",
   "metadata": {},
   "source": [
    "You can easily check by direct multiplication that the result is correct.  It should be clear from this example though why we do NOT solve $Ax=b$ by finding $A^{-1}$ and then forming $x=A^{-1}b$.  Finding $A^{-1}$ requires factorization ($\\sim n^3/3$ flops) and then solving $n$ linear systems using the factorization (each on cost $n^2$ flops and there are $n$ of them, so $n^3$ flops).  That is significantly more work than just solving $Ax=b$ directly and even if we want to solve for multiple $b$'s the matrix multiplication to form $x=A^{-1}b$ cost just as much as solving for $x$ using the $LU$ factorization of $A$.  \n",
    "\n",
    "Note that our algorithm choice of having $1$'s on the diagonal of $L$ is not unique.  That particular choice is referred to as *Dolittle* factorization.  The observant reader would have noticed that the initial example of an $LU$ factorized matrix that we gave at the top of this page actually had $1$'s on the diagonal of $U$ while the diagonal of $L$ had nonunit entries, a choice known as *Crout*'s factorization.  Another choice, *Cholesky*'s factorization, has diagonal entries of $U$, $u_{kk}$, and diagonal elements of $L$, $l_{kk}$ that are equal, i.e. $l_{kk}=u_{kk}$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
